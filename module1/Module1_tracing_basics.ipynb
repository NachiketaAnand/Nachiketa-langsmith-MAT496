{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "RAG_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "The pre-existing conversation may provide important context to the question.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Conversation: {conversation}\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "def get_vector_db_retriever():\n",
        "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
        "    embd = OpenAIEmbeddings()\n",
        "\n",
        "    # If vector store exists, then load it\n",
        "    if os.path.exists(persist_path):\n",
        "        vectorstore = SKLearnVectorStore(\n",
        "            embedding=embd,\n",
        "            persist_path=persist_path,\n",
        "            serializer=\"parquet\"\n",
        "        )\n",
        "        return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "    # Otherwise, index LangSmith documents and create new vector store\n",
        "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
        "    ls_docs = ls_docs_sitemap_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=500, chunk_overlap=0\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(ls_docs)\n",
        "\n",
        "    vectorstore = SKLearnVectorStore.from_documents(\n",
        "        documents=doc_splits,\n",
        "        embedding=embd,\n",
        "        persist_path=persist_path,\n",
        "        serializer=\"parquet\"\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "    return vectorstore.as_retriever(lambda_mult=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRDPqhKt5yjs",
        "outputId": "d7cb8818-9a35-4669-bf77-fbb2a7075b6b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ],
      "metadata": {
        "id": "eP3Gg5vMhmg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langgraph-sdk langgraph-checkpoint-sqlite \"langsmith>=0.2.0\" langchain-community langchain-core langchain-openai notebook python-dotenv lxml scikit-learn pandas pyarrow utils\n"
      ],
      "metadata": {
        "id": "MTH-1EGwiLyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "from utils import get_vector_db_retriever\n",
        "from langsmith.run_helpers import traceable\n",
        "\n",
        "MODEL_PROVIDER = \"openai\"\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "APP_VERSION = 1.0\n",
        "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\"\"\"\n",
        "\n",
        "openai_client = OpenAI()\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n",
        "\n",
        "@traceable\n",
        "def retrieve_documents(question: str, langsmith_extra=None):\n",
        "    return retriever.invoke(question)\n",
        "\n",
        "\n",
        "@traceable\n",
        "def generate_response(question: str, documents, langsmith_extra=None):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": RAG_SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
        "        }\n",
        "    ]\n",
        "    return call_openai(messages)\n",
        "\n",
        "\n",
        "@traceable\n",
        "def call_openai(\n",
        "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0, langsmith_extra=None\n",
        ") -> str:\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "@traceable\n",
        "def langsmith_rag(question: str, langsmith_extra=None):\n",
        "    documents = retrieve_documents(question, langsmith_extra=langsmith_extra)\n",
        "    response = generate_response(question, documents, langsmith_extra=langsmith_extra)\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "7yqr25zhhu6i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How do I add Metadata to a Run with @traceable?\"\n",
        "ai_answer = langsmith_rag(question)\n",
        "print(ai_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCO3mNXy6Njj",
        "outputId": "0ae141c1-9ea4-4853-8540-6129c6246e2a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can add metadata to a run by including a metadata key named `ls_run_name` in the `experimental_telemetry` object when calling the `generateText` function. For example, you can structure your call like this: \n",
            "\n",
            "```javascript\n",
            "await generateText({\n",
            "  model: openai(\"gpt-4.1-nano\"),\n",
            "  prompt: \"Your prompt here.\",\n",
            "  experimental_telemetry: {\n",
            "    isEnabled: true,\n",
            "    metadata: { userId: \"123\", language: \"english\" },\n",
            "  },\n",
            "});\n",
            "```\n",
            "\n",
            "This metadata will be visible in your LangSmith dashboard and can be used for filtering and searching traces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How do I add metadata at runtime?\"\n",
        "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
        "print(ai_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj_WzgvW8evd",
        "outputId": "f0a8764a-8892-4486-d646-a464c12d6fa5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To add metadata at runtime, you can attach it when you run an experiment in the SDK. This involves specifying the metadata related to your experiment, such as the model used or other relevant identifiers. Ensure that the same metadata is also attached to the trace for consistent filtering and analysis later.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E3zEyF5G6NVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "hlLKZ8PJ88y6"
      }
    }
  ]
}