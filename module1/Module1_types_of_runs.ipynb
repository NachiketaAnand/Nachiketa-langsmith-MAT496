{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "RAG_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "The pre-existing conversation may provide important context to the question.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Conversation: {conversation}\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "def get_vector_db_retriever():\n",
        "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
        "    embd = OpenAIEmbeddings()\n",
        "\n",
        "    # If vector store exists, then load it\n",
        "    if os.path.exists(persist_path):\n",
        "        vectorstore = SKLearnVectorStore(\n",
        "            embedding=embd,\n",
        "            persist_path=persist_path,\n",
        "            serializer=\"parquet\"\n",
        "        )\n",
        "        return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "    # Otherwise, index LangSmith documents and create new vector store\n",
        "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
        "    ls_docs = ls_docs_sitemap_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=500, chunk_overlap=0\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(ls_docs)\n",
        "\n",
        "    vectorstore = SKLearnVectorStore.from_documents(\n",
        "        documents=doc_splits,\n",
        "        embedding=embd,\n",
        "        persist_path=persist_path,\n",
        "        serializer=\"parquet\"\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "    return vectorstore.as_retriever(lambda_mult=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRDPqhKt5yjs",
        "outputId": "d7cb8818-9a35-4669-bf77-fbb2a7075b6b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ],
      "metadata": {
        "id": "eP3Gg5vMhmg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langgraph-sdk langgraph-checkpoint-sqlite \"langsmith>=0.2.0\" langchain-community langchain-core langchain-openai notebook python-dotenv lxml scikit-learn pandas pyarrow utils\n"
      ],
      "metadata": {
        "id": "MTH-1EGwiLyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "inputs = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},\n",
        "]\n",
        "\n",
        "output = {\n",
        "  \"choices\": [\n",
        "      {\n",
        "          \"message\": {\n",
        "              \"role\": \"assistant\",\n",
        "              \"content\": \"Sure, what time would you like to book the table for?\"\n",
        "          }\n",
        "      }\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Can also use one of:\n",
        "# output = {\n",
        "#     \"message\": {\n",
        "#         \"role\": \"assistant\",\n",
        "#         \"content\": \"Sure, what time would you like to book the table for?\"\n",
        "#     }\n",
        "# }\n",
        "#\n",
        "# output = {\n",
        "#     \"role\": \"assistant\",\n",
        "#     \"content\": \"Sure, what time would you like to book the table for?\"\n",
        "# }\n",
        "#\n",
        "# output = [\"assistant\", \"Sure, what time would you like to book the table for?\"]\n",
        "\n",
        "@traceable(\n",
        "  # TODO: Add an run_type=\"llm\", and metadata for ls_provider, and ls_model_name\n",
        ")\n",
        "def chat_model(messages: list):\n",
        "  return output\n",
        "\n",
        "chat_model(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yqr25zhhu6i",
        "outputId": "0cd598f4-5148-47c7-87a4-37a3d65b15f7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'choices': [{'message': {'role': 'assistant',\n",
              "    'content': 'Sure, what time would you like to book the table for?'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _reduce_chunks(chunks: list):\n",
        "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
        "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
        "\n",
        "@traceable(\n",
        "    run_type=\"llm\",\n",
        "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"},\n",
        "    # TODO: Add a reduce_fn\n",
        ")\n",
        "def my_streaming_chat_model(messages: list):\n",
        "    for chunk in [\"Hello, \" + messages[1][\"content\"]]:\n",
        "        yield {\n",
        "            \"choices\": [\n",
        "                {\n",
        "                    \"message\": {\n",
        "                        \"content\": chunk,\n",
        "                        \"role\": \"assistant\",\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "list(\n",
        "    my_streaming_chat_model(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},\n",
        "            {\"role\": \"user\", \"content\": \"polly the parrot\"},\n",
        "        ],\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCO3mNXy6Njj",
        "outputId": "c6cedb61-5652-4828-81e8-51e0765be579"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'choices': [{'message': {'content': 'Hello, polly the parrot',\n",
              "     'role': 'assistant'}}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import traceable\n",
        "\n",
        "def _convert_docs(results):\n",
        "  return [\n",
        "      {\n",
        "          \"page_content\": r,\n",
        "          \"type\": \"Document\", # This is the wrong format! The key should be type\n",
        "          \"metadata\": {\"foo\": \"bar\"}\n",
        "      }\n",
        "      for r in results\n",
        "  ]\n",
        "\n",
        "@traceable(\n",
        "    # TODO: Add an run_type=\"retriever\"\n",
        ")\n",
        "def retrieve_docs(query):\n",
        "  # Retriever returning hardcoded dummy documents.\n",
        "  # In production, this could be a real vector datatabase or other document index.\n",
        "  contents = [\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"]\n",
        "  return _convert_docs(contents)\n",
        "\n",
        "retrieve_docs(\"User query\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj_WzgvW8evd",
        "outputId": "790cf027-8f1b-4851-bbc8-e8e74bddeb83"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_content': 'Document contents 1',\n",
              "  'type': 'Document',\n",
              "  'metadata': {'foo': 'bar'}},\n",
              " {'page_content': 'Document contents 2',\n",
              "  'type': 'Document',\n",
              "  'metadata': {'foo': 'bar'}},\n",
              " {'page_content': 'Document contents 3',\n",
              "  'type': 'Document',\n",
              "  'metadata': {'foo': 'bar'}}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E3zEyF5G6NVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import traceable\n",
        "from openai import OpenAI\n",
        "from typing import List, Optional\n",
        "import json\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "@traceable(\n",
        "  # TODO: Add an run_type=\"tool\"\n",
        ")\n",
        "def get_current_temperature(location: str, unit: str):\n",
        "    return 65 if unit == \"Fahrenheit\" else 17\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def call_openai(\n",
        "    messages: List[dict], tools: Optional[List[dict]]\n",
        ") -> str:\n",
        "  return openai_client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    tools=tools\n",
        "  )\n",
        "\n",
        "@traceable(run_type=\"chain\")\n",
        "def ask_about_the_weather(inputs, tools):\n",
        "  response = call_openai(inputs, tools)\n",
        "  tool_call_args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
        "  location = tool_call_args[\"location\"]\n",
        "  unit = tool_call_args[\"unit\"]\n",
        "  tool_response_message = {\n",
        "    \"role\": \"tool\",\n",
        "    \"content\": json.dumps({\n",
        "        \"location\": location,\n",
        "        \"unit\": unit,\n",
        "        \"temperature\": get_current_temperature(location, unit),\n",
        "    }),\n",
        "    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
        "  }\n",
        "  inputs.append(response.choices[0].message)\n",
        "  inputs.append(tool_response_message)\n",
        "  output = call_openai(inputs, None)\n",
        "  return output\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "      \"type\": \"function\",\n",
        "      \"function\": {\n",
        "        \"name\": \"get_current_temperature\",\n",
        "        \"description\": \"Get the current temperature for a specific location\",\n",
        "        \"parameters\": {\n",
        "          \"type\": \"object\",\n",
        "          \"properties\": {\n",
        "            \"location\": {\n",
        "              \"type\": \"string\",\n",
        "              \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
        "            },\n",
        "            \"unit\": {\n",
        "              \"type\": \"string\",\n",
        "              \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
        "              \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
        "            }\n",
        "          },\n",
        "          \"required\": [\"location\", \"unit\"]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "]\n",
        "inputs = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"What is the weather today in New York City?\"},\n",
        "]\n",
        "\n",
        "ask_about_the_weather(inputs, tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SaIwu91O32S",
        "outputId": "8bd69787-9823-4174-f6c4-f2ea7c32dcfe"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-CN0COkzrdJ6edFAx8KQRbB0diHnvn', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in New York City is 65Â°F. If you need more detailed weather information, feel free to ask!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759598344, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=25, prompt_tokens=83, total_tokens=108, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "hlLKZ8PJ88y6"
      }
    }
  ]
}