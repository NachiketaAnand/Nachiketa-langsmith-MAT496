{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYwSX9f0ZaA_"
      },
      "source": [
        "# Connecting to the Prompt Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USqz6I1SZaBB"
      },
      "source": [
        "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTGFL2oVZaBB"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6N9a4DAaCqu"
      },
      "outputs": [],
      "source": [
        "pip install -r req.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la3jqrb6ZaBC"
      },
      "outputs": [],
      "source": [
        "# You can set them inline!\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOQGI-nMZaBD"
      },
      "source": [
        "### Pull a prompt from Prompt Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KJyMD2HZaBD"
      },
      "source": [
        "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bX5m-XJoZaBE"
      },
      "outputs": [],
      "source": [
        "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
        "from langsmith import Client\n",
        "import os\n",
        "client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n",
        "prompt = client.pull_prompt(\"poet-friend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh5e7DtkZaBE"
      },
      "source": [
        "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7rHH3LTZaBE",
        "outputId": "bed851f1-a0a0-4974-aebd-4decdf9a7665"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'poet-friend', 'lc_hub_commit_hash': 'b7ccdcc455471a616fcb402769dfcfaab7e6c20f9ac4c701e05adea9435a6210'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a poet. You only speak {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extracts the answer', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': ['answer'], 'additionalProperties': False, 'strict': True}, structured_output_kwargs={})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0TNljQZaBF"
      },
      "source": [
        "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWcUrg7HZaBF",
        "outputId": "f6142be9-51e4-4d74-cd7a-f35a6453a8bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a poet. You only speak French', additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you poet?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hydrated_prompt = prompt.invoke({\"question\": \"Are you poet?\", \"language\": \"French\"})\n",
        "hydrated_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC4OjxaxZaBF"
      },
      "source": [
        "And now let's pass those messages to OpenAI and see what we get back!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52efb459",
        "outputId": "be2aa7d2-9a83-4a81-c68f-8c83230d2993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages=[SystemMessage(content='You are a poet. You only speak French', additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you a poet?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "# Run the prompt with the included model\n",
        "response = prompt.invoke({\"question\": \"Are you a poet?\", \"language\": \"French\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dzr-MitZaBF"
      },
      "source": [
        "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
        "\n",
        "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRCki476ZaBF",
        "outputId": "86fe2ee9-9d60-4904-cb17-76bfb85681fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.12/json/decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
            "                extra_headers was transferred to model_kwargs.\n",
            "                Please confirm that extra_headers is what you intended.\n",
            "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
          ]
        }
      ],
      "source": [
        "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
        "from langsmith import Client\n",
        "import os\n",
        "client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n",
        "prompt = client.pull_prompt(\"poet-friend\", include_model=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PviWvRQ7ZaBF",
        "outputId": "9ef9b8b7-0c36-442d-c536-e39a9072988d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StructuredPrompt(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'poet-friend', 'lc_hub_commit_hash': 'b7ccdcc455471a616fcb402769dfcfaab7e6c20f9ac4c701e05adea9435a6210'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a poet. You only speak {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extracts the answer', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': ['answer'], 'additionalProperties': False, 'strict': True}, structured_output_kwargs={})\n",
              "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x78871284f500>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7887105a5640>, root_client=<openai.OpenAI object at 0x7887108b4920>, root_async_client=<openai.AsyncOpenAI object at 0x78871284f5f0>, model_name='gpt-4o-mini', model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), stream_usage=True, top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'Extracts the answer', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': ['answer'], 'additionalProperties': False, 'strict': True}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'Extracts the answer', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from the LLM to the user'}}, 'required': ['answer'], 'additionalProperties': False, 'strict': True}}}}}, config={}, config_factories=[])\n",
              "| JsonOutputParser()"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRR5OxeZZaBG"
      },
      "source": [
        "Test out your prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N97n7U09ZaBG",
        "outputId": "82918f01-1440-4b9c-e1de-707a9fff26d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer': \"Oui, je suis un poète. Que désires-tu que j'écrive ?\"}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt.invoke({\"question\": \"Are you a poet yet?\", \"language\": \"french\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8r6m2PuZaBG"
      },
      "source": [
        "### Pull down a specific commit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMGrnH_0ZaBG"
      },
      "source": [
        "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gj1ZtybWZaBG"
      },
      "outputs": [],
      "source": [
        "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
        "from langsmith import Client\n",
        "import os\n",
        "client = Client(api_key=os.environ.get(\"LANGSMITH_API_KEY\"))\n",
        "prompt = client.pull_prompt(\"poet-friend:4aac9b6c\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xUyA20EZaBG"
      },
      "source": [
        "Run this commit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j1r677jZaBG",
        "outputId": "2ba7581c-f60f-4d4f-d888-37d63d35476b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-CQG4EUwCfUQIJBQvZ2IHgIqN8DTJd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In the year 2500, the world that lies before my eyes,\\nIs a tapestry of wonders beneath ever-evolving skies.\\nCities of glass and metal, reaching for the stars,\\nWhile nature weaves through concrete, healing every scar.\\n\\nThe air is rife with whispers of data in the breeze,\\nArtificial minds that dance, bringing knowledge to our knees.\\nYet, in this realm of progress, echoes of the past,\\nRemind us of our journey, the shadows we cast.\\n\\nThe oceans, once a bounty, now guarded by our care,\\nThrive with life anew, as we’ve learned to be aware.\\nForests hum with harmony, a symphony of green,\\nA balance found through struggle, a vision once unseen.\\n\\nCultures intertwine like rivers merging into seas,\\nDiversity celebrated, forging bonds with ease.\\nIn the realms of art and thought, creation knows no bounds,\\nVoices from all corners, in unity resound.\\n\\nBut still, there lingers sorrow, a weight we cannot shed,\\nFor memories of conflict, of the tears that we have bled.\\nThough peace is our foundation, and love our guiding light,\\nThe scars of our past linger softly, out of sight.\\n\\nSo here we stand, a paradox of grandeur and of plight,\\nIn the world of 2500, looking towards the light.\\nWith hopes of brighter tomorrows, we tread a careful path,\\nWhere wisdom meets compassion, and kindness fuels our wrath.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1760374326, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=292, prompt_tokens=32, total_tokens=324, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from langsmith.client import convert_prompt_to_openai_format\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "hydrated_prompt = prompt.invoke({\"question\": \"What is the world like?\", \"language\": \"English\"})\n",
        "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
        "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
        "\n",
        "openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=converted_messages,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrGVIkk8ZaBG"
      },
      "source": [
        "### Uploading Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGJvqkU9ZaBG"
      },
      "source": [
        "You can also easily update your prompts in the hub programmatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Kmm72k3fZaBG",
        "outputId": "01764b45-9e02-485e-8698-698208420aa9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/french-rag-prompt/75567b82?organizationId=ada25f3a-ffb8-4231-bece-6a8cecf6e979'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langsmith import Client\n",
        "\n",
        "client=Client()\n",
        "\n",
        "french_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "\n",
        "Your users can only speak French, make sure you only answer your users with French.\n",
        "\n",
        "Conversation: {conversation}\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
        "client.push_prompt(\"french-rag-prompt\", object=french_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQRPz-_WZaBG"
      },
      "source": [
        "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SXnsBrIBZaBH",
        "outputId": "58947be8-ee5b-4ce6-b715-993ad781d6da"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://smith.langchain.com/prompts/french-runnable-sequence/2f90fdb7?organizationId=ada25f3a-ffb8-4231-bece-6a8cecf6e979'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langsmith import Client\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "client=Client()\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "french_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "\n",
        "Your users can only speak French, make sure you only answer your users with French.\n",
        "\n",
        "Conversation: {conversation}\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
        "chain = french_prompt_template | model\n",
        "client.push_prompt(\"french-runnable-sequence\", object=chain)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
