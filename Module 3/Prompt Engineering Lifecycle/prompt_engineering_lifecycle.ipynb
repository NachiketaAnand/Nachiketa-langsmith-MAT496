{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AculmACxo-rM"
      },
      "source": [
        "# Prompt Engineering Lifecycle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pv9pSKjo-rO"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvAH7bqTpijV",
        "outputId": "3be8c7fa-a70f-4495-a80c-165e402cd684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph (from -r req.txt (line 1))\n",
            "  Downloading langgraph-0.6.10-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langgraph-sdk (from -r req.txt (line 2))\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langgraph-checkpoint-sqlite (from -r req.txt (line 3))\n",
            "  Downloading langgraph_checkpoint_sqlite-2.0.11-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langsmith>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 4)) (0.4.33)\n",
            "Collecting langchain-community (from -r req.txt (line 5))\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 6)) (0.3.78)\n",
            "Collecting langchain-openai (from -r req.txt (line 7))\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 8)) (6.5.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 9)) (1.1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 10)) (5.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 11)) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 12)) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from -r req.txt (line 13)) (18.1.0)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph->-r req.txt (line 1))\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph->-r req.txt (line 1))\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r req.txt (line 1)) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r req.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk->-r req.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk->-r req.txt (line 2)) (3.11.3)\n",
            "Collecting aiosqlite>=0.20 (from langgraph-checkpoint-sqlite->-r req.txt (line 3))\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting sqlite-vec>=0.1.6 (from langgraph-checkpoint-sqlite->-r req.txt (line 3))\n",
            "  Downloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl.metadata (198 bytes)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.2.0->-r req.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.2.0->-r req.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.2.0->-r req.txt (line 4)) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.2.0->-r req.txt (line 4)) (0.25.0)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (2.0.43)\n",
            "Collecting requests>=2.0.0 (from langsmith>=0.2.0->-r req.txt (line 4))\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community->-r req.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r req.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r req.txt (line 6)) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core->-r req.txt (line 6)) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->-r req.txt (line 7)) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->-r req.txt (line 7)) (0.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (3.1.6)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (6.4.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (26.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (25.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (5.8.1)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (7.4.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (7.16.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (1.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (6.17.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (0.23.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook->-r req.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r req.txt (line 11)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r req.txt (line 11)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r req.txt (line 11)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r req.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r req.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r req.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r req.txt (line 5)) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r req.txt (line 5))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r req.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk->-r req.txt (line 2)) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk->-r req.txt (line 2)) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk->-r req.txt (line 2)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk->-r req.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk->-r req.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->-r req.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client<8,>=5.3.4->notebook->-r req.txt (line 8)) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.6.1->notebook->-r req.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community->-r req.txt (line 5)) (0.3.11)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph->-r req.txt (line 1))\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook->-r req.txt (line 8)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook->-r req.txt (line 8)) (2.19.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->-r req.txt (line 8)) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook->-r req.txt (line 8)) (4.25.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->-r req.txt (line 7)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->-r req.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->-r req.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai->-r req.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph->-r req.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph->-r req.txt (line 1)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph->-r req.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r req.txt (line 12)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.2.0->-r req.txt (line 4)) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.2.0->-r req.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community->-r req.txt (line 5)) (3.2.4)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.12/dist-packages (from terminado>=0.8.3->notebook->-r req.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->-r req.txt (line 7)) (2024.11.6)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook->-r req.txt (line 8)) (25.1.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->notebook->-r req.txt (line 8)) (1.8.15)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->notebook->-r req.txt (line 8)) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->notebook->-r req.txt (line 8)) (0.1.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->notebook->-r req.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook->-r req.txt (line 8)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook->-r req.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (3.0.52)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r req.txt (line 8)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r req.txt (line 8)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook->-r req.txt (line 8)) (0.27.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (2.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r req.txt (line 5))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->-r req.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook->-r req.txt (line 8)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->-r req.txt (line 8)) (2.23)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (0.8.5)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->notebook->-r req.txt (line 8)) (0.2.14)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (20.11.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->-r req.txt (line 8)) (2.9.0.20251008)\n",
            "Downloading langgraph-0.6.10-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint_sqlite-2.0.11-py3-none-any.whl (31 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlite_vec-0.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux1_x86_64.whl (151 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: sqlite-vec, requests, ormsgpack, mypy-extensions, marshmallow, jedi, aiosqlite, typing-inspect, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph-checkpoint-sqlite, langgraph, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiosqlite-0.21.0 dataclasses-json-0.6.7 jedi-0.19.2 langchain-community-0.3.31 langchain-openai-0.3.35 langgraph-0.6.10 langgraph-checkpoint-2.1.2 langgraph-checkpoint-sqlite-2.0.11 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.11.0 requests-2.32.5 sqlite-vec-0.1.6 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install -r req.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z52M5Lk9o-rP"
      },
      "outputs": [],
      "source": [
        "# You can set them inline!\n",
        "import os\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0SsVAW0o-rP"
      },
      "outputs": [],
      "source": [
        "# Or you can use a .env file\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(dotenv_path=\"../../.env\", override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnqcs5Mfo-rQ"
      },
      "source": [
        "### Log a trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "yu5rAhY6o-rQ",
        "outputId": "6b26dedd-efc8-4f1b-d73b-d423f76c031d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Fetching pages: 100%|##########| 197/197 [00:27<00:00,  7.18it/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"To set up tracing to LangSmith with @traceable, ensure you set the LANGSMITH_TRACING environment variable to 'true' and the LANGSMITH_API_KEY to your API key. Then, simply decorate any function with @traceable in your Python code. This allows the function to log traces automatically when called.\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from app import langsmith_rag\n",
        "\n",
        "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
        "langsmith_rag(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWIxH7kPo-rQ"
      },
      "source": [
        "### Create a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wDNz0Kto-rQ"
      },
      "source": [
        "Let's create a dataset to evaluate this particular step of our application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF1rYIWmo-rR",
        "outputId": "fa9056a6-58cd-4a56-9e63-9668088a2313"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['f6c181fe-aaba-4456-a0c7-c1e76093be7a',\n",
              "  '178e0c6e-4d5c-465c-b561-afb21ebac40e',\n",
              "  '5201f3a2-ae21-424d-bb9e-9ed6d2c5fd24'],\n",
              " 'count': 3}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "example_dataset = [\n",
        "    (\n",
        "        \"How do I set up tracing to LangSmith with @traceable?\",\n",
        "        \"\"\" 2. Log a traceâ€‹\\nOnce you've set up your environment, you can call LangChain runnables as normal.\\nLangSmith will infer the proper tracing config:\\n\\nHow to log and view traces to LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith\\n\\n2. Log a traceâ€‹\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingAnnotate code for tracingOn this pageAnnotate code for tracing\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceableâ€‹\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nnoteThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section. \\n\\n\"\"\",\n",
        "        \"To set up tracing to LangSmith using the `@traceable` decorator in Python, first ensure that you have the `LANGSMITH_TRACING` environment variable set to 'true' and the `LANGSMITH_API_KEY` set to your API key. Then, you can simply decorate your functions like this:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n```\\n\\nThis will log traces for `my_function` automatically once the necessary environment variables are configured.\"\n",
        "    ),\n",
        "    (\n",
        "        \"How can I use the tracing context manager?\",\n",
        "        \"\"\"Use the trace context manager (Python only)â€‹\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nIn some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nRecently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\\nwe changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\nPythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingTroubleshoot trace nestingOn this pageTroubleshoot trace nesting\\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \\\"edge cases\\\".\\nPythonâ€‹\\nThe following outlines common causes for \\\"split\\\" traces when building with python.\\nContext propagation using asyncioâ€‹\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's asyncio only added full support for passing context in version 3.11.\\nWhyâ€‹\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\nTo resolveâ€‹\\n\\nContext propagation using threadingâ€‹\\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib ThreadPoolExecutor by default breaks tracing.\\nWhyâ€‹\\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\nTo resolveâ€‹\\n\\n\\nUsing LangSmith's ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter. \\n\\n \"\"\",\n",
        "        \"You can use the tracing context manager by wrapping the code you want to trace with the `with trace_context:` statement. Here's a simple example:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Your traceable code here\\n    print(\\\"Tracing this block of code.\\\")\\n``` \\n\\nThis will log traces to LangSmith for the specified code block.\"\n",
        "    ),\n",
        "    (\n",
        "        \"How can I use RunTree?\",\n",
        "        \"\"\"PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-4o-mini\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI Call\\\",\\n\\nPythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})await pipeline.postRun();# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-3.5-turbo\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI\\n\\nAlternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\n\\nRunTree({  run_type: \\\"llm\\\",  name: \\\"OpenAI Call RunTree\\\",  inputs: { messages },  tags: [\\\"my-tag\\\"],  extra: {metadata: {\\\"my-key\\\": \\\"my-value\\\"}}})await rt.postRun();const chatCompletion = await client.chat.completions.create({  model: \\\"gpt-3.5-turbo\\\",  messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"You are a helpful AI.\\\"),(\\\"user\\\", \\\"{input}\\\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}})# Tags and metadata can also be passed at runtimechain.invoke({\\\"input\\\": \\\"What is the meaning of life?\\\"}, {\\\"tags\\\": [\\\"shared-tags\\\"], \\\"metadata\\\": {\\\"shared-key\\\": \\\"shared-value\\\"}})import { ChatOpenAI } from \\\"@langchain/openai\\\";import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";const prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"You are a helpful AI.\\\"],[\\\"user\\\", \\\"{input}\\\"]])const model = new ChatOpenAI({ modelName: \\\"gpt-3.5-turbo\\\" });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}});// Tags and metadata can also be \\n\\n\"\"\",\n",
        "        \"You can use `RunTree` by creating a pipeline where you define its name, type, and inputs. For example:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"My Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Your question here\\\"})\\n```\\n\\nYou can then create child runs and manage them accordingly within the pipeline.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "dataset_name = \"Technical Questions\"\n",
        "\n",
        "# Create dataset\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"Technical questions about LangSmith\"\n",
        ")\n",
        "\n",
        "# Prepare inputs and outputs\n",
        "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
        "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
        "\n",
        "# Create examples in the dataset\n",
        "client.create_examples(\n",
        "    inputs=inputs,\n",
        "    outputs=outputs,\n",
        "    dataset_id=dataset.id,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94aLAvuBo-rS"
      },
      "source": [
        "### Update our Application to use Prompt Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8oB-m5No-rS"
      },
      "source": [
        "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
        "\n",
        "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
        "\n",
        "Let's add the code snippet that will pull down our prompt that we just iterated on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U57yyCzLo-rS"
      },
      "outputs": [],
      "source": [
        "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
        "import os\n",
        "from langsmith import Client\n",
        "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
        "prompt = client.pull_prompt(\"rag_with_code\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4857e169"
      },
      "outputs": [],
      "source": [
        "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
        "langsmith_rag(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XkzuILfYo-rS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langsmith import traceable\n",
        "from langsmith.client import convert_prompt_to_openai_format\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import nest_asyncio\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "MODEL_PROVIDER = \"openai\"\n",
        "APP_VERSION = 1.0\n",
        "\n",
        "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
        "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\"\"\"\n",
        "\n",
        "openai_client = OpenAI()\n",
        "\n",
        "def get_vector_db_retriever():\n",
        "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
        "    embd = OpenAIEmbeddings()\n",
        "\n",
        "    # If vector store exists, then load it\n",
        "    if os.path.exists(persist_path):\n",
        "        vectorstore = SKLearnVectorStore(\n",
        "            embedding=embd,\n",
        "            persist_path=persist_path,\n",
        "            serializer=\"parquet\"\n",
        "        )\n",
        "        return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "    # Otherwise, index LangSmith documents and create new vector store\n",
        "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
        "    ls_docs = ls_docs_sitemap_loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=500, chunk_overlap=0\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(ls_docs)\n",
        "\n",
        "    vectorstore = SKLearnVectorStore.from_documents(\n",
        "        documents=doc_splits,\n",
        "        embedding=embd,\n",
        "        persist_path=persist_path,\n",
        "        serializer=\"parquet\"\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "    return vectorstore.as_retriever(lambda_mult=0)\n",
        "\n",
        "nest_asyncio.apply()\n",
        "retriever = get_vector_db_retriever()\n",
        "\n",
        "\"\"\"\n",
        "retrieve_documents\n",
        "- Returns documents fetched from a vectorstore based on the user's question\n",
        "\"\"\"\n",
        "@traceable(run_type=\"chain\")\n",
        "def retrieve_documents(question: str):\n",
        "    return retriever.invoke(question)\n",
        "\n",
        "\"\"\"\n",
        "generate_response\n",
        "- Calls `call_openai` to generate a model response after formatting inputs\n",
        "\"\"\"\n",
        "@traceable(run_type=\"chain\")\n",
        "def generate_response(question: str, documents):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
        "    # TODO: Let's use our prompt pulled from Prompt Hub instead of manually formatting here!\n",
        "\n",
        "    formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
        "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
        "    return call_openai(messages)\n",
        "\n",
        "\"\"\"\n",
        "call_openai\n",
        "- Returns the chat completion output from OpenAI\n",
        "\"\"\"\n",
        "@traceable(\n",
        "    run_type=\"llm\",\n",
        "    metadata={\n",
        "        \"ls_provider\": MODEL_PROVIDER,\n",
        "        \"ls_model_name\": MODEL_NAME\n",
        "    }\n",
        ")\n",
        "def call_openai(messages: List[dict]) -> str:\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "\"\"\"\n",
        "langsmith_rag\n",
        "- Calls `retrieve_documents` to fetch documents\n",
        "- Calls `generate_response` to generate a response based on the fetched documents\n",
        "- Returns the model response\n",
        "\"\"\"\n",
        "@traceable(run_type=\"chain\")\n",
        "def langsmith_rag(question: str):\n",
        "    documents = retrieve_documents(question)\n",
        "    response = generate_response(question, documents)\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "D6IDdZQwo-rS",
        "outputId": "a58637bd-fc7e-489f-87e1-7be3d23ed31b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"To set up tracing to LangSmith with the `@traceable` decorator, first ensure you have the `LANGSMITH_TRACING` environment variable set to 'true' and `LANGSMITH_API_KEY` set to your API key. Then, simply decorate your function with `@traceable`. Hereâ€™s a basic example:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n``` \\n\\nThis will log traces for `my_function` when executed.\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
        "langsmith_rag(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "K1_TwmErul8Z",
        "outputId": "6c5ee310-4d22-4f35-90e3-da540e809226"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You can print \"Hello, world!\" in Python using the following simple code:\\n\\n```python\\nprint(\"Hello, world!\")\\n```\\n\\nJust run this line in your Python environment to see the output.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How do I print Hello world in python?\"\n",
        "langsmith_rag(question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ls-academy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
